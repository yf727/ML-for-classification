{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1:\n",
    "\n",
    "Implement the KNN classifier.\n",
    "\n",
    "Your implementation should accept two data files as input (both are posted with the assignment): a spam train.csv file (weka spam train.arff for Weka users) and a spam test.csv file (weka spam test.arff for Weka users). Both files contain exam- ples of e-mail messages, with each example having a class label of either “1” (spam) or “0” (no-spam). Each example has 57 (numeric) features that characterize the message. Your classifier should examine each example in the spam test set and classify it as one of the two classes. The classification will be based on an unweighted vote of its k nearest examples in the spam train set. We will measure all distances using regular Euclidean distance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>f1</th>\n",
       "      <th>f2</th>\n",
       "      <th>f3</th>\n",
       "      <th>f4</th>\n",
       "      <th>f5</th>\n",
       "      <th>f6</th>\n",
       "      <th>f7</th>\n",
       "      <th>f8</th>\n",
       "      <th>f9</th>\n",
       "      <th>f10</th>\n",
       "      <th>...</th>\n",
       "      <th>f49</th>\n",
       "      <th>f50</th>\n",
       "      <th>f51</th>\n",
       "      <th>f52</th>\n",
       "      <th>f53</th>\n",
       "      <th>f54</th>\n",
       "      <th>f55</th>\n",
       "      <th>f56</th>\n",
       "      <th>f57</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.778</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.756</td>\n",
       "      <td>61</td>\n",
       "      <td>278</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.21</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.94</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.132</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.372</td>\n",
       "      <td>0.180</td>\n",
       "      <td>0.048</td>\n",
       "      <td>5.114</td>\n",
       "      <td>101</td>\n",
       "      <td>1028</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.06</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.71</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.23</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.25</td>\n",
       "      <td>...</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.143</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.276</td>\n",
       "      <td>0.184</td>\n",
       "      <td>0.010</td>\n",
       "      <td>9.821</td>\n",
       "      <td>485</td>\n",
       "      <td>2259</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.63</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.137</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.137</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.537</td>\n",
       "      <td>40</td>\n",
       "      <td>191</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.63</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.135</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.135</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.537</td>\n",
       "      <td>40</td>\n",
       "      <td>191</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 58 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     f1    f2    f3   f4    f5    f6    f7    f8    f9   f10  ...     f49  \\\n",
       "0  0.00  0.64  0.64  0.0  0.32  0.00  0.00  0.00  0.00  0.00  ...    0.00   \n",
       "1  0.21  0.28  0.50  0.0  0.14  0.28  0.21  0.07  0.00  0.94  ...    0.00   \n",
       "2  0.06  0.00  0.71  0.0  1.23  0.19  0.19  0.12  0.64  0.25  ...    0.01   \n",
       "3  0.00  0.00  0.00  0.0  0.63  0.00  0.31  0.63  0.31  0.63  ...    0.00   \n",
       "4  0.00  0.00  0.00  0.0  0.63  0.00  0.31  0.63  0.31  0.63  ...    0.00   \n",
       "\n",
       "     f50  f51    f52    f53    f54    f55  f56   f57  class  \n",
       "0  0.000  0.0  0.778  0.000  0.000  3.756   61   278      1  \n",
       "1  0.132  0.0  0.372  0.180  0.048  5.114  101  1028      1  \n",
       "2  0.143  0.0  0.276  0.184  0.010  9.821  485  2259      1  \n",
       "3  0.137  0.0  0.137  0.000  0.000  3.537   40   191      1  \n",
       "4  0.135  0.0  0.135  0.000  0.000  3.537   40   191      1  \n",
       "\n",
       "[5 rows x 58 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "spam_train = pd.read_csv('Desktop/code/CISC6930_DM/data/spam_train.csv')\n",
    "spam_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>f1</th>\n",
       "      <th>f2</th>\n",
       "      <th>f3</th>\n",
       "      <th>f4</th>\n",
       "      <th>f5</th>\n",
       "      <th>f6</th>\n",
       "      <th>f7</th>\n",
       "      <th>f8</th>\n",
       "      <th>f9</th>\n",
       "      <th>...</th>\n",
       "      <th>f49</th>\n",
       "      <th>f50</th>\n",
       "      <th>f51</th>\n",
       "      <th>f52</th>\n",
       "      <th>f53</th>\n",
       "      <th>f54</th>\n",
       "      <th>f55</th>\n",
       "      <th>f56</th>\n",
       "      <th>f57</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>t1</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2.50</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.182</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.182</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.545</td>\n",
       "      <td>21</td>\n",
       "      <td>78</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>t2</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.63</td>\n",
       "      <td>1.91</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.42</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.169</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.358</td>\n",
       "      <td>0.188</td>\n",
       "      <td>0.075</td>\n",
       "      <td>2.847</td>\n",
       "      <td>75</td>\n",
       "      <td>447</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>t3</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.37</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.242</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.145</td>\n",
       "      <td>9.584</td>\n",
       "      <td>332</td>\n",
       "      <td>508</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>t4</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.96</td>\n",
       "      <td>0.98</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.96</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.168</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.011</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>2.888</td>\n",
       "      <td>12</td>\n",
       "      <td>52</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>t5</td>\n",
       "      <td>0.51</td>\n",
       "      <td>0.43</td>\n",
       "      <td>0.29</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.18</td>\n",
       "      <td>0.54</td>\n",
       "      <td>...</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.080</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.454</td>\n",
       "      <td>0.523</td>\n",
       "      <td>0.136</td>\n",
       "      <td>6.590</td>\n",
       "      <td>739</td>\n",
       "      <td>2333</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 59 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   ID    f1    f2    f3    f4    f5    f6    f7    f8    f9  ...      f49  \\\n",
       "0  t1  0.00  0.00  0.00  0.00  0.00  0.00  0.00  2.50  0.00  ...    0.000   \n",
       "1  t2  0.31  0.00  0.63  1.91  0.21  0.00  0.00  0.00  0.42  ...    0.000   \n",
       "2  t3  0.00  0.75  0.37  0.00  0.00  0.00  0.75  0.00  0.00  ...    0.000   \n",
       "3  t4  0.00  1.96  0.98  0.00  0.00  0.00  1.96  0.00  0.00  ...    0.000   \n",
       "4  t5  0.51  0.43  0.29  0.00  0.14  0.03  0.00  0.18  0.54  ...    0.012   \n",
       "\n",
       "     f50  f51    f52    f53    f54    f55  f56   f57  Label  \n",
       "0  0.182  0.0  0.000  0.182  0.000  3.545   21    78      1  \n",
       "1  0.169  0.0  0.358  0.188  0.075  2.847   75   447      1  \n",
       "2  0.000  0.0  0.242  0.000  0.145  9.584  332   508      1  \n",
       "3  0.168  0.0  1.011  0.000  0.000  2.888   12    52      1  \n",
       "4  0.080  0.0  0.454  0.523  0.136  6.590  739  2333      1  \n",
       "\n",
       "[5 rows x 59 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spam_test = pd.read_csv('Desktop/code/CISC6930_DM/data/spam_test.csv')\n",
    "spam_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "\n",
    "# create design matrix X and target vector y\n",
    "X_train = np.array(spam_train.iloc[:, 0:56])\n",
    "y_train = np.array(spam_train['class'])\n",
    "\n",
    "X_test = np.array(spam_test.iloc[:, 1:57])\n",
    "y_test = np.array(spam_test['Label'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (a) \n",
    "\n",
    "Report test accuracies when k=1,5,11,21,41,61,81,101,201,401 without normalizing the features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Training method: memorizing the training data\n",
    "def train(X_train, y_train):\n",
    "    # do nothing\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# prediction method: compute the euclidean distance, then select the K nearest ones\n",
    "# then perform majority vote\n",
    "# then assign the labels to instances\n",
    "\n",
    "from collections import Counter\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def predict(X_train, y_train, X_test, k):\n",
    "    # create list for distances and targets\n",
    "    distances = []\n",
    "    targets = []\n",
    "    \n",
    "    for i in range(len(X_train)):\n",
    "        # first compute the euclidean distance\n",
    "        distance = np.sqrt(np.sum(np.square(X_test - X_train[i, :])))\n",
    "        # add it to the list of distances\n",
    "        distances.append([distance, i])\n",
    "    \n",
    "    # sort the list\n",
    "    distances = sorted(distances)\n",
    "    \n",
    "    # make a list of k neighbors' targets\n",
    "    for i in range(k):\n",
    "        index = distances[i][1]\n",
    "        targets.append(y_train[index])\n",
    "    \n",
    "    # return most common target: majority vote \n",
    "    return Counter(targets).most_common(1)[0][0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The accuracy of Our classifier is 83%\n"
     ]
    }
   ],
   "source": [
    "def kNearestNeighbor(X_train, y_train, X_test, predictions, k):\n",
    "    # check if k larger than n\n",
    "    if k > len(X_train):\n",
    "        raise ValueError\n",
    "    \n",
    "    # train on the input data\n",
    "    train(X_train, y_train)\n",
    "    \n",
    "    # predict for each testing observation \n",
    "    for i in range(len(X_test)):\n",
    "        predictions.append(predict(X_train, y_train, X_test[i, :], k))\n",
    "\n",
    "# make predictions\n",
    "predictions = []\n",
    "try:\n",
    "        kNearestNeighbor(X_train, y_train, X_test, predictions, 1)\n",
    "        predictions = np.asarray(predictions)\n",
    "        \n",
    "        # evaluating accuracy \n",
    "        accuracy = accuracy_score(y_test, predictions) * 100\n",
    "        print('\\nThe accuracy of Our classifier is %d%%' % accuracy)\n",
    "        \n",
    "except ValueError:\n",
    "        print('Can\\'t have more neighbors than training samples!!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The accuracy of Our classifier is 83%\n"
     ]
    }
   ],
   "source": [
    "def kNearestNeighbor(X_train, y_train, X_test, predictions, k):\n",
    "    # check if k larger than n\n",
    "    if k > len(X_train):\n",
    "        raise ValueError\n",
    "    \n",
    "    # train on the input data\n",
    "    train(X_train, y_train)\n",
    "    \n",
    "    # predict for each testing observation \n",
    "    for i in range(len(X_test)):\n",
    "        predictions.append(predict(X_train, y_train, X_test[i, :], k))\n",
    "\n",
    "# make predictions\n",
    "predictions = []\n",
    "try:\n",
    "        kNearestNeighbor(X_train, y_train, X_test, predictions, 5)\n",
    "        predictions = np.asarray(predictions)\n",
    "        \n",
    "        # evaluating accuracy \n",
    "        accuracy = accuracy_score(y_test, predictions) * 100\n",
    "        print('\\nThe accuracy of Our classifier is %d%%' % accuracy)\n",
    "        \n",
    "except ValueError:\n",
    "        print('Can\\'t have more neighbors than training samples!!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The accuracy of Our classifier is 83%\n"
     ]
    }
   ],
   "source": [
    "def kNearestNeighbor(X_train, y_train, X_test, predictions, k):\n",
    "    # check if k larger than n\n",
    "    if k > len(X_train):\n",
    "        raise ValueError\n",
    "    \n",
    "    # train on the input data\n",
    "    train(X_train, y_train)\n",
    "    \n",
    "    # predict for each testing observation \n",
    "    for i in range(len(X_test)):\n",
    "        predictions.append(predict(X_train, y_train, X_test[i, :], k))\n",
    "\n",
    "# make predictions\n",
    "predictions = []\n",
    "try:\n",
    "        kNearestNeighbor(X_train, y_train, X_test, predictions, 11)\n",
    "        predictions = np.asarray(predictions)\n",
    "        \n",
    "        # evaluating accuracy \n",
    "        accuracy = accuracy_score(y_test, predictions) * 100\n",
    "        print('\\nThe accuracy of Our classifier is %d%%' % accuracy)\n",
    "        \n",
    "except ValueError:\n",
    "        print('Can\\'t have more neighbors than training samples!!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The accuracy of Our classifier is 84%\n"
     ]
    }
   ],
   "source": [
    "def kNearestNeighbor(X_train, y_train, X_test, predictions, k):\n",
    "    # check if k larger than n\n",
    "    if k > len(X_train):\n",
    "        raise ValueError\n",
    "    \n",
    "    # train on the input data\n",
    "    train(X_train, y_train)\n",
    "    \n",
    "    # predict for each testing observation \n",
    "    for i in range(len(X_test)):\n",
    "        predictions.append(predict(X_train, y_train, X_test[i, :], k))\n",
    "\n",
    "# make predictions\n",
    "predictions = []\n",
    "try:\n",
    "        kNearestNeighbor(X_train, y_train, X_test, predictions, 21)\n",
    "        predictions = np.asarray(predictions)\n",
    "        \n",
    "        # evaluating accuracy \n",
    "        accuracy = accuracy_score(y_test, predictions) * 100\n",
    "        print('\\nThe accuracy of Our classifier is %d%%' % accuracy)\n",
    "        \n",
    "except ValueError:\n",
    "        print('Can\\'t have more neighbors than training samples!!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The accuracy of Our classifier is 82%\n"
     ]
    }
   ],
   "source": [
    "def kNearestNeighbor(X_train, y_train, X_test, predictions, k):\n",
    "    # check if k larger than n\n",
    "    if k > len(X_train):\n",
    "        raise ValueError\n",
    "    \n",
    "    # train on the input data\n",
    "    train(X_train, y_train)\n",
    "    \n",
    "    # predict for each testing observation \n",
    "    for i in range(len(X_test)):\n",
    "        predictions.append(predict(X_train, y_train, X_test[i, :], k))\n",
    "\n",
    "# make predictions\n",
    "predictions = []\n",
    "try:\n",
    "        kNearestNeighbor(X_train, y_train, X_test, predictions, 41)\n",
    "        predictions = np.asarray(predictions)\n",
    "        \n",
    "        # evaluating accuracy \n",
    "        accuracy = accuracy_score(y_test, predictions) * 100\n",
    "        print('\\nThe accuracy of Our classifier is %d%%' % accuracy)\n",
    "        \n",
    "except ValueError:\n",
    "        print('Can\\'t have more neighbors than training samples!!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The accuracy of Our classifier is 82%\n"
     ]
    }
   ],
   "source": [
    "def kNearestNeighbor(X_train, y_train, X_test, predictions, k):\n",
    "    # check if k larger than n\n",
    "    if k > len(X_train):\n",
    "        raise ValueError\n",
    "    \n",
    "    # train on the input data\n",
    "    train(X_train, y_train)\n",
    "    \n",
    "    # predict for each testing observation \n",
    "    for i in range(len(X_test)):\n",
    "        predictions.append(predict(X_train, y_train, X_test[i, :], k))\n",
    "\n",
    "# make predictions\n",
    "predictions = []\n",
    "try:\n",
    "        kNearestNeighbor(X_train, y_train, X_test, predictions, 61)\n",
    "        predictions = np.asarray(predictions)\n",
    "        \n",
    "        # evaluating accuracy \n",
    "        accuracy = accuracy_score(y_test, predictions) * 100\n",
    "        print('\\nThe accuracy of Our classifier is %d%%' % accuracy)\n",
    "        \n",
    "except ValueError:\n",
    "        print('Can\\'t have more neighbors than training samples!!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The accuracy of Our classifier is 82%\n"
     ]
    }
   ],
   "source": [
    "def kNearestNeighbor(X_train, y_train, X_test, predictions, k):\n",
    "    # check if k larger than n\n",
    "    if k > len(X_train):\n",
    "        raise ValueError\n",
    "    \n",
    "    # train on the input data\n",
    "    train(X_train, y_train)\n",
    "    \n",
    "    # predict for each testing observation \n",
    "    for i in range(len(X_test)):\n",
    "        predictions.append(predict(X_train, y_train, X_test[i, :], k))\n",
    "\n",
    "# make predictions\n",
    "predictions = []\n",
    "try:\n",
    "        kNearestNeighbor(X_train, y_train, X_test, predictions, 81)\n",
    "        predictions = np.asarray(predictions)\n",
    "        \n",
    "        # evaluating accuracy \n",
    "        accuracy = accuracy_score(y_test, predictions) * 100\n",
    "        print('\\nThe accuracy of Our classifier is %d%%' % accuracy)\n",
    "        \n",
    "except ValueError:\n",
    "        print('Can\\'t have more neighbors than training samples!!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The accuracy of Our classifier is 81%\n"
     ]
    }
   ],
   "source": [
    "def kNearestNeighbor(X_train, y_train, X_test, predictions, k):\n",
    "    # check if k larger than n\n",
    "    if k > len(X_train):\n",
    "        raise ValueError\n",
    "    \n",
    "    # train on the input data\n",
    "    train(X_train, y_train)\n",
    "    \n",
    "    # predict for each testing observation \n",
    "    for i in range(len(X_test)):\n",
    "        predictions.append(predict(X_train, y_train, X_test[i, :], k))\n",
    "\n",
    "# make predictions\n",
    "predictions = []\n",
    "try:\n",
    "        kNearestNeighbor(X_train, y_train, X_test, predictions, 101)\n",
    "        predictions = np.asarray(predictions)\n",
    "        \n",
    "        # evaluating accuracy \n",
    "        accuracy = accuracy_score(y_test, predictions) * 100\n",
    "        print('\\nThe accuracy of Our classifier is %d%%' % accuracy)\n",
    "        \n",
    "except ValueError:\n",
    "        print('Can\\'t have more neighbors than training samples!!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The accuracy of Our classifier is 79%\n"
     ]
    }
   ],
   "source": [
    "def kNearestNeighbor(X_train, y_train, X_test, predictions, k):\n",
    "    # check if k larger than n\n",
    "    if k > len(X_train):\n",
    "        raise ValueError\n",
    "    \n",
    "    # train on the input data\n",
    "    train(X_train, y_train)\n",
    "    \n",
    "    # predict for each testing observation \n",
    "    for i in range(len(X_test)):\n",
    "        predictions.append(predict(X_train, y_train, X_test[i, :], k))\n",
    "\n",
    "# make predictions\n",
    "predictions = []\n",
    "try:\n",
    "        kNearestNeighbor(X_train, y_train, X_test, predictions, 201)\n",
    "        predictions = np.asarray(predictions)\n",
    "        \n",
    "        # evaluating accuracy \n",
    "        accuracy = accuracy_score(y_test, predictions) * 100\n",
    "        print('\\nThe accuracy of Our classifier is %d%%' % accuracy)\n",
    "        \n",
    "except ValueError:\n",
    "        print('Can\\'t have more neighbors than training samples!!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The accuracy of Our classifier is 73%\n"
     ]
    }
   ],
   "source": [
    "def kNearestNeighbor(X_train, y_train, X_test, predictions, k):\n",
    "    # check if k larger than n\n",
    "    if k > len(X_train):\n",
    "        raise ValueError\n",
    "    \n",
    "    # train on the input data\n",
    "    train(X_train, y_train)\n",
    "    \n",
    "    # predict for each testing observation \n",
    "    for i in range(len(X_test)):\n",
    "        predictions.append(predict(X_train, y_train, X_test[i, :], k))\n",
    "\n",
    "# make predictions\n",
    "predictions = []\n",
    "try:\n",
    "        kNearestNeighbor(X_train, y_train, X_test, predictions, 401)\n",
    "        predictions = np.asarray(predictions)\n",
    "        \n",
    "        # evaluating accuracy \n",
    "        accuracy = accuracy_score(y_test, predictions) * 100\n",
    "        print('\\nThe accuracy of Our classifier is %d%%' % accuracy)\n",
    "        \n",
    "except ValueError:\n",
    "        print('Can\\'t have more neighbors than training samples!!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.8300738809213386,\n",
       " 0.834419817470665,\n",
       " 0.837896566710126,\n",
       " 0.8426770969143851,\n",
       " 0.8265971316818774,\n",
       " 0.8209474141677532,\n",
       " 0.8226857887874837,\n",
       " 0.8105171664493699,\n",
       " 0.7957409821816601,\n",
       " 0.737070838765754]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# now we compare the function and results with sklearn\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# create a list of K for KNN\n",
    "k_neighbors = [1,5,11,21,41,61,81,101,201,401]\n",
    "\n",
    "# create a empty list to hold accuracy score\n",
    "accuracy_scores = []\n",
    "\n",
    "# fit KNN on train and test data \n",
    "for k in k_neighbors:\n",
    "    knn = KNeighborsClassifier(n_neighbors = k)\n",
    "    knn.fit(X_train, y_train)\n",
    "    pred = knn.predict(X_test)\n",
    "    scores = accuracy_score(y_test, pred)\n",
    "    accuracy_scores.append(scores)\n",
    "\n",
    "accuracy_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer: as we can see, the KNN function produces the same accuracy rate as the sklearn package"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (b) \n",
    "\n",
    "Report test accuracies when k=1,5,11,21,41,61,81,101,201,401 with z-score normalization applied to the features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "\n",
    "X_trainz = preprocessing.StandardScaler(X_train)\n",
    "X_testz = preprocessing.StandardScaler(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create a list of K for KNN\n",
    "k_neighbors = [1,5,11,21,41,61,81,101,201,401]\n",
    "\n",
    "# create a empty list to hold accuracy score\n",
    "accuracy_scores_2 = []\n",
    "\n",
    "# fit KNN on train and test data \n",
    "for k in k_neighbors:\n",
    "    knn = KNeighborsClassifier(n_neighbors = k)\n",
    "    knn.fit(X_trainz, y_train)\n",
    "    pred_z = knn.predict(X_testz)\n",
    "    scores_z = accuracy_score(y_test, pred)\n",
    "    accuracy_scores_2.append(scores)\n",
    "\n",
    "accuracy_scores_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (c) \n",
    "\n",
    "In the (b) case, generate an output of KNN predicted labels for the first 50 instances (i.e. t1 - t50) when k = 1, 5, 11, 21, 41, 61, 81, 101, 201, 401 (in this order). \n",
    "\n",
    "For example, if t5 is classified as class ‘spam” when k = 1,5,11,21,41,61 and classified as class “no-spam” when k = 81, 101, 201, 401, then your output line for t5 should be:\n",
    "\n",
    "t5 spam, spam, spam, spam, spam, spam, no, no, no, no\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'preprocessing' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-c33efb1123c8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mX_test_50\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreprocessing\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscale\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspam_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m57\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# create a list of K for KNN\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mk_neighbors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m11\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m21\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m41\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m61\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m81\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m101\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m201\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m401\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'preprocessing' is not defined"
     ]
    }
   ],
   "source": [
    "X_test_50 = preprocessing.scale(np.array(spam_test.iloc[0:50, 1:57]))\n",
    "\n",
    "# create a list of K for KNN\n",
    "k_neighbors = [1,5,11,21,41,61,81,101,201,401]\n",
    "\n",
    "# intialize a dataframe to store the results\n",
    "ind = spam_test.iloc[0:50, 0]\n",
    "output_matrix = pd.DataFrame(index = ind, columns = k_neighbors)\n",
    "\n",
    "# fit KNN on train and test data \n",
    "for k in k_neighbors:\n",
    "    knn_50 = KNeighborsClassifier(n_neighbors = k)\n",
    "    knn_50.fit(X_trainz, y_train)\n",
    "    pred_50 = knn_50.predict(X_test_50)\n",
    "\n",
    "pred_50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (d) What can you conclude by comparing the KNN performance in (a) and (b)?\n",
    "\n",
    "Answer: \n",
    "\n",
    "After standardization, the accuracy rate for the KNN in (a) and (b) becomes the same. \n",
    "\n",
    "From this result, we can conclude that the units/scales of the features in the original data was different and they shouldn't be measured on the same scale. Without standardization, the features with bigger unit tend to have bigger weight which have big impact on the predictions. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (e) Describe a method to select the optimal k for the KNN algorithm.\n",
    "\n",
    "Answer: \n",
    "\n",
    "we could use cross-validation to select the optimal k. \n",
    "\n",
    "Here I performed 10-fold cross-validation by seperating the training data into 10 equal sized folds. Each time, we train the data on 9 fold, and estimate the error on the 10th fold that was left. We repeat thi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## perform 10-fold Cross-Validation to select K\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# create a list of K for KNN\n",
    "neighbors = list(range(1,50))\n",
    "\n",
    "# empty list that will hold cv scores\n",
    "cv_scores = []\n",
    "\n",
    "# perform 10-fold cross validation\n",
    "for k in neighbors:\n",
    "    knn = KNeighborsClassifier(n_neighbors = k)\n",
    "    scores = cross_val_score(knn, X_trainz, y_train, cv = 10, scoring = 'accuracy')\n",
    "    cv_scores.append(scores.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The optimal number of neighbors is 11\n"
     ]
    }
   ],
   "source": [
    "# find the minimum misclassfication error\n",
    "MSE = [1 - x for x in cv_scores]\n",
    "min(MSE)\n",
    "optimal_k = neighbors[MSE.index(min(MSE))]\n",
    "\n",
    "print (\"The optimal number of neighbors is %d\" % optimal_k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZIAAAEKCAYAAAA4t9PUAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10d325198>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(neighbors, MSE)\n",
    "plt.xlabel('Number of Neighbors K')\n",
    "plt.ylabel('Misclassification Error')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
